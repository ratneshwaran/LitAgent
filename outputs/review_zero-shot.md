## Executive summary

Reviewed 10 papers across 7 venues. We summarize methods, results, and limitations, and identify common gaps.

## Comparative matrix

| Paper | Venue | Year | Citations | Methods | Results | Limitations |
| --- | --- | --- | --- | --- | --- | --- |
| Task Contamination: Language Models May Not Be Few-Shot Anymore | arXiv | 2023 |  |  |  |  |
| Projected Subnetworks Scale Adaptation | arXiv | 2023 |  |  |  |  |
| GLiREL -- Generalist Model for Zero-Shot Relation Extraction | arXiv | 2025 |  |  |  |  |
| Improving Audio Classification by Transitioning from Zero- to Few-Shot | arXiv | 2025 |  |  |  |  |
| Development of anatomical metrics for objective quantification of critical view of safety. | Surgical endoscopy | 2025 |  |  |  |  |
| Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models. | Translational vision science & technology | 2025 |  |  |  |  |
| BatGPT-Chem: A Foundation Large Model for Chemical Engineering. | Research (Washington, D.C.) | 2025 |  |  |  |  |
| Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes. | Cell systems | 2025 |  |  |  |  |
| araCNA: somatic copy number profiling using long-range sequence models. | NAR genomics and bioinformatics | 2025 |  |  |  |  |
| EEG-CLIP: learning EEG representations from natural language descriptions. | Frontiers in robotics and AI | 2025 |  |  |  |  |


## Mini-reviews

### Task Contamination: Language Models May Not Be Few-Shot Anymore

arXiv, 2023, Changmao Li; Jeffrey Flanigan

**TL;DR**

Large language models (LLMs) offer impressive performance in various
zero-shot and few-shot tasks. However, their success in zero-shot and few-shot
settings may be affected by task contamination, a potential limitation that has
not been thoroughly examined. This paper investigate...

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

**Grounding quotes**

> Large language models (LLMs) offer impressive performance in various
zero-shot and few-shot tasks. However, their success in zero-shot and few-shot
settings may be affected by task contamination, a po (abstract)

### Projected Subnetworks Scale Adaptation

arXiv, 2023, Siddhartha Datta; Nigel Shadbolt

**TL;DR**

Large models support great zero-shot and few-shot capabilities. However,
updating these models on new tasks can break performance on previous seen tasks
and their zero/few-shot unseen tasks. Our work explores how to update
zero/few-shot learners such that they can maintain perfor...

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

**Grounding quotes**

> Large models support great zero-shot and few-shot capabilities. However,
updating these models on new tasks can break performance on previous seen tasks
and their zero/few-shot unseen tasks (abstract)

### GLiREL -- Generalist Model for Zero-Shot Relation Extraction

arXiv, 2025, Jack Boylan; Chris Hokamp; Demian Gholipour Ghalandari

**TL;DR**

We introduce GLiREL (Generalist Lightweight model for zero-shot Relation
Extraction), an efficient architecture and training paradigm for zero-shot
relation classification. Inspired by recent advancements in zero-shot named
entity recognition, this work presents an approach to ef...

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

**Grounding quotes**

> We introduce GLiREL (Generalist Lightweight model for zero-shot Relation
Extraction), an efficient architecture and training paradigm for zero-shot
relation classification. Inspired by recent advancem (abstract)

### Improving Audio Classification by Transitioning from Zero- to Few-Shot

arXiv, 2025, James Taylor; Wolfgang Mack

**TL;DR**

State-of-the-art audio classification often employs a zero-shot approach,
which involves comparing audio embeddings with embeddings from text describing
the respective audio class. These embeddings are usually generated by neural
networks trained through contrastive learning to a...

**Critique**

- overclaiming: Claims strong superiority; check against baselines in paper. (severity: medium)
- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

**Grounding quotes**

> State-of-the-art audio classification often employs a zero-shot approach,
which involves comparing audio embeddings with embeddings from text describing
the respective audio class. These embeddings ar (abstract)

### Development of anatomical metrics for objective quantification of critical view of safety.

Surgical endoscopy, 2025, Mueller B; Mlambo B; Kulason S; Nespolo R; Guo R

**TL;DR**

Development of anatomical metrics for objective quantification of critical view of safety.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

### Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.

Translational vision science & technology, 2025, Nguyen Q; Nguyen DA; Dang K; Liu S; Wang SY; Woof WA; Thomas PBM; Patel PJ; Balaskas K; Thygesen JH; Wu H; Pontikos N

**TL;DR**

Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

### BatGPT-Chem: A Foundation Large Model for Chemical Engineering.

Research (Washington, D.C.), 2025, Yang Y; Shi R; Li Z; Jiang S; Lu BL; Zhao Q; Yang Y; Zhao H

**TL;DR**

BatGPT-Chem: A Foundation Large Model for Chemical Engineering.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

### Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.

Cell systems, 2025, Li FZ; Yang J; Johnston KE; Gürsoy E; Yue Y; Arnold FH

**TL;DR**

Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

### araCNA: somatic copy number profiling using long-range sequence models.

NAR genomics and bioinformatics, 2025, Visscher E; Yau C

**TL;DR**

araCNA: somatic copy number profiling using long-range sequence models.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

### EEG-CLIP: learning EEG representations from natural language descriptions.

Frontiers in robotics and AI, 2025, Camaret Ndir T; Schirrmeister RT; Ball T

**TL;DR**

EEG-CLIP: learning EEG representations from natural language descriptions.

**Critique**

- missing_baselines: Results do not clearly compare against established baselines. (severity: low)
- reproducibility: No mention of code/data availability or reproducibility. (severity: medium)

## Gaps & future work

- Few works report comparisons against strong baselines.
- Open-sourcing code and datasets
- Larger, diverse cohorts
- Robust baseline comparisons

### References

- Changmao Li, Jeffrey Flanigan. (2023) Task Contamination: Language Models May Not Be Few-Shot Anymore. arXiv. http://arxiv.org/abs/2312.16337v1
- Siddhartha Datta, Nigel Shadbolt. (2023) Projected Subnetworks Scale Adaptation. arXiv. http://arxiv.org/abs/2301.11487v1
- Jack Boylan, Chris Hokamp, Demian Gholipour Ghalandari. (2025) GLiREL -- Generalist Model for Zero-Shot Relation Extraction. arXiv. http://arxiv.org/abs/2501.03172v1
- James Taylor, Wolfgang Mack. (2025) Improving Audio Classification by Transitioning from Zero- to Few-Shot. arXiv. http://arxiv.org/abs/2507.20036v1
- Mueller B, Mlambo B, Kulason S, Nespolo R, Guo R. (2025) Development of anatomical metrics for objective quantification of critical view of safety.. Surgical endoscopy. https://pubmed.ncbi.nlm.nih.gov/40940617/
- Nguyen Q, Nguyen DA, Dang K, Liu S, Wang SY, Woof WA, Thomas PBM, Patel PJ, Balaskas K, Thygesen JH, Wu H, Pontikos N. (2025) Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.. Translational vision science & technology. https://pubmed.ncbi.nlm.nih.gov/40938068/
- Yang Y, Shi R, Li Z, Jiang S, Lu BL, Zhao Q, Yang Y, Zhao H. (2025) BatGPT-Chem: A Foundation Large Model for Chemical Engineering.. Research (Washington, D.C.). https://pubmed.ncbi.nlm.nih.gov/40936797/
- Li FZ, Yang J, Johnston KE, Gürsoy E, Yue Y, Arnold FH. (2025) Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.. Cell systems. https://pubmed.ncbi.nlm.nih.gov/40934912/
- Visscher E, Yau C. (2025) araCNA: somatic copy number profiling using long-range sequence models.. NAR genomics and bioinformatics. https://pubmed.ncbi.nlm.nih.gov/40933674/
- Camaret Ndir T, Schirrmeister RT, Ball T. (2025) EEG-CLIP: learning EEG representations from natural language descriptions.. Frontiers in robotics and AI. https://pubmed.ncbi.nlm.nih.gov/40933652/

