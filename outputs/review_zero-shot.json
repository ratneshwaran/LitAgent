{
  "topic": "zero shot",
  "filters": {
    "start_year": null,
    "end_year": null,
    "include_keywords": [],
    "exclude_keywords": [],
    "venues": [],
    "limit": 10
  },
  "raw_papers": [
    {
      "id": "http://arxiv.org/abs/2312.16337v1",
      "source": "arxiv",
      "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
      "abstract": "Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.",
      "authors": [
        "Changmao Li",
        "Jeffrey Flanigan"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2312.16337v1",
      "pdf_url": "http://arxiv.org/pdf/2312.16337v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2301.11487v1",
      "source": "arxiv",
      "title": "Projected Subnetworks Scale Adaptation",
      "abstract": "Large models support great zero-shot and few-shot capabilities. However,\nupdating these models on new tasks can break performance on previous seen tasks\nand their zero/few-shot unseen tasks. Our work explores how to update\nzero/few-shot learners such that they can maintain performance on seen/unseen\ntasks of previous tasks as well as new tasks. By manipulating the parameter\nupdates of a gradient-based meta learner as the projected task-specific\nsubnetworks, we show improvements for large models to retain seen and zero/few\nshot task performance in online settings.",
      "authors": [
        "Siddhartha Datta",
        "Nigel Shadbolt"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2301.11487v1",
      "pdf_url": "http://arxiv.org/pdf/2301.11487v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2501.03172v1",
      "source": "arxiv",
      "title": "GLiREL -- Generalist Model for Zero-Shot Relation Extraction",
      "abstract": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancements in zero-shot named\nentity recognition, this work presents an approach to efficiently and\naccurately predict zero-shot relationship labels between multiple entities in a\nsingle forward pass. Experiments using the FewRel and WikiZSL benchmarks\ndemonstrate that our approach achieves state-of-the-art results on the\nzero-shot relation classification task. In addition, we contribute a protocol\nfor synthetically-generating datasets with diverse relation labels.",
      "authors": [
        "Jack Boylan",
        "Chris Hokamp",
        "Demian Gholipour Ghalandari"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2501.03172v1",
      "pdf_url": "http://arxiv.org/pdf/2501.03172v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2507.20036v1",
      "source": "arxiv",
      "title": "Improving Audio Classification by Transitioning from Zero- to Few-Shot",
      "abstract": "State-of-the-art audio classification often employs a zero-shot approach,\nwhich involves comparing audio embeddings with embeddings from text describing\nthe respective audio class. These embeddings are usually generated by neural\nnetworks trained through contrastive learning to align audio and text\nrepresentations. Identifying the optimal text description for an audio class is\nchallenging, particularly when the class comprises a wide variety of sounds.\nThis paper examines few-shot methods designed to improve classification\naccuracy beyond the zero-shot approach. Specifically, audio embeddings are\ngrouped by class and processed to replace the inherently noisy text embeddings.\nOur results demonstrate that few-shot classification typically outperforms the\nzero-shot baseline.",
      "authors": [
        "James Taylor",
        "Wolfgang Mack"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2507.20036v1",
      "pdf_url": "http://arxiv.org/pdf/2507.20036v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40940617",
      "source": "pubmed",
      "title": "Development of anatomical metrics for objective quantification of critical view of safety.",
      "abstract": null,
      "authors": [
        "Mueller B",
        "Mlambo B",
        "Kulason S",
        "Nespolo R",
        "Guo R"
      ],
      "year": 2025,
      "venue": "Surgical endoscopy",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40940617/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40938068",
      "source": "pubmed",
      "title": "Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.",
      "abstract": null,
      "authors": [
        "Nguyen Q",
        "Nguyen DA",
        "Dang K",
        "Liu S",
        "Wang SY",
        "Woof WA",
        "Thomas PBM",
        "Patel PJ",
        "Balaskas K",
        "Thygesen JH",
        "Wu H",
        "Pontikos N"
      ],
      "year": 2025,
      "venue": "Translational vision science & technology",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40938068/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40936797",
      "source": "pubmed",
      "title": "BatGPT-Chem: A Foundation Large Model for Chemical Engineering.",
      "abstract": null,
      "authors": [
        "Yang Y",
        "Shi R",
        "Li Z",
        "Jiang S",
        "Lu BL",
        "Zhao Q",
        "Yang Y",
        "Zhao H"
      ],
      "year": 2025,
      "venue": "Research (Washington, D.C.)",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40936797/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40934912",
      "source": "pubmed",
      "title": "Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.",
      "abstract": null,
      "authors": [
        "Li FZ",
        "Yang J",
        "Johnston KE",
        "GÃ¼rsoy E",
        "Yue Y",
        "Arnold FH"
      ],
      "year": 2025,
      "venue": "Cell systems",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40934912/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40933674",
      "source": "pubmed",
      "title": "araCNA: somatic copy number profiling using long-range sequence models.",
      "abstract": null,
      "authors": [
        "Visscher E",
        "Yau C"
      ],
      "year": 2025,
      "venue": "NAR genomics and bioinformatics",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40933674/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40933652",
      "source": "pubmed",
      "title": "EEG-CLIP: learning EEG representations from natural language descriptions.",
      "abstract": null,
      "authors": [
        "Camaret Ndir T",
        "Schirrmeister RT",
        "Ball T"
      ],
      "year": 2025,
      "venue": "Frontiers in robotics and AI",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40933652/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    }
  ],
  "reviews": [
    {
      "paper_id": "http://arxiv.org/abs/2312.16337v1",
      "summary": {
        "tldr": "Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigate...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a po",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2301.11487v1",
      "summary": {
        "tldr": "Large models support great zero-shot and few-shot capabilities. However,\nupdating these models on new tasks can break performance on previous seen tasks\nand their zero/few-shot unseen tasks. Our work explores how to update\nzero/few-shot learners such that they can maintain perfor...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Large models support great zero-shot and few-shot capabilities. However,\nupdating these models on new tasks can break performance on previous seen tasks\nand their zero/few-shot unseen tasks",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2501.03172v1",
      "summary": {
        "tldr": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancements in zero-shot named\nentity recognition, this work presents an approach to ef...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancem",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2507.20036v1",
      "summary": {
        "tldr": "State-of-the-art audio classification often employs a zero-shot approach,\nwhich involves comparing audio embeddings with embeddings from text describing\nthe respective audio class. These embeddings are usually generated by neural\nnetworks trained through contrastive learning to a...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "State-of-the-art audio classification often employs a zero-shot approach,\nwhich involves comparing audio embeddings with embeddings from text describing\nthe respective audio class. These embeddings ar",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "overclaiming",
            "severity": "medium",
            "rationale": "Claims strong superiority; check against baselines in paper."
          },
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40940617",
      "summary": {
        "tldr": "Development of anatomical metrics for objective quantification of critical view of safety.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40938068",
      "summary": {
        "tldr": "Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40936797",
      "summary": {
        "tldr": "BatGPT-Chem: A Foundation Large Model for Chemical Engineering.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40934912",
      "summary": {
        "tldr": "Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40933674",
      "summary": {
        "tldr": "araCNA: somatic copy number profiling using long-range sequence models.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40933652",
      "summary": {
        "tldr": "EEG-CLIP: learning EEG representations from natural language descriptions.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    }
  ],
  "matrix": {
    "columns": [
      "Venue",
      "Year",
      "Citations",
      "Methods",
      "Results",
      "Limitations"
    ],
    "rows": [
      "Task Contamination: Language Models May Not Be Few-Shot Anymore",
      "Projected Subnetworks Scale Adaptation",
      "GLiREL -- Generalist Model for Zero-Shot Relation Extraction",
      "Improving Audio Classification by Transitioning from Zero- to Few-Shot",
      "Development of anatomical metrics for objective quantification of critical view of safety.",
      "Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.",
      "BatGPT-Chem: A Foundation Large Model for Chemical Engineering.",
      "Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.",
      "araCNA: somatic copy number profiling using long-range sequence models.",
      "EEG-CLIP: learning EEG representations from natural language descriptions."
    ],
    "data": {
      "Task Contamination: Language Models May Not Be Few-Shot Anymore": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Projected Subnetworks Scale Adaptation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "GLiREL -- Generalist Model for Zero-Shot Relation Extraction": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Improving Audio Classification by Transitioning from Zero- to Few-Shot": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Development of anatomical metrics for objective quantification of critical view of safety.": {
        "Venue": {
          "value": "Surgical endoscopy"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Advancing Question-Answering in Ophthalmology With Retrieval-Augmented Generation: Benchmarking Open-Source and Proprietary Large Language Models.": {
        "Venue": {
          "value": "Translational vision science & technology"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "BatGPT-Chem: A Foundation Large Model for Chemical Engineering.": {
        "Venue": {
          "value": "Research (Washington, D.C.)"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Evaluation of machine learning-assisted directed evolution across diverse combinatorial landscapes.": {
        "Venue": {
          "value": "Cell systems"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "araCNA: somatic copy number profiling using long-range sequence models.": {
        "Venue": {
          "value": "NAR genomics and bioinformatics"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "EEG-CLIP: learning EEG representations from natural language descriptions.": {
        "Venue": {
          "value": "Frontiers in robotics and AI"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      }
    }
  },
  "synthesis": {
    "executive_summary": "Reviewed 10 papers across 7 venues. We summarize methods, results, and limitations, and identify common gaps.",
    "gaps": [
      "Few works report comparisons against strong baselines."
    ],
    "future_work": [
      "Open-sourcing code and datasets",
      "Larger, diverse cohorts",
      "Robust baseline comparisons"
    ]
  },
  "artifacts": {
    "markdown_path": "outputs\\review_zero-shot.md",
    "json_path": "outputs\\review_zero-shot.json",
    "csv_path": "outputs\\papers_zero-shot.csv"
  },
  "created_at": "2025-09-13T06:21:23.029076"
}