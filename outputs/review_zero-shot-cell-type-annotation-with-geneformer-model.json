{
  "topic": "zero shot cell type annotation with geneformer model",
  "filters": {
    "start_year": null,
    "end_year": null,
    "include_keywords": [],
    "exclude_keywords": [],
    "venues": [],
    "limit": 10
  },
  "raw_papers": [
    {
      "id": "http://arxiv.org/abs/2411.06331v1",
      "source": "arxiv",
      "title": "GenoHoption: Bridging Gene Network Graphs and Single-Cell Foundation\n  Models",
      "abstract": "The remarkable success of foundation models has sparked growing interest in\ntheir application to single-cell biology. Models like Geneformer and scGPT\npromise to serve as versatile tools in this specialized field. However,\nrepresenting a cell as a sequence of genes remains an open question since the\norder of genes is interchangeable. Injecting the gene network graph offers gene\nrelative positions and compact data representation but poses a dilemma: limited\nreceptive fields without in-layer message passing or parameter explosion with\nmessage passing in each layer. To pave the way forward, we propose GenoHoption,\na new computational framework for single-cell sequencing data that effortlessly\ncombines the strengths of these foundation models with explicit relationships\nin gene networks. We also introduce a constraint that lightens the model by\nfocusing on learning the predefined graph structure while ensuring further hops\nare deducted to expand the receptive field. Empirical studies show that our\nmodel improves by an average of 1.27% on cell-type annotation and 3.86% on\nperturbation prediction. Furthermore, our method significantly decreases\ncomputational overhead and exhibits few-shot potential. GenoHoption can\nfunction as an efficient and expressive bridge, connecting existing single-cell\nfoundation models to gene network graphs.",
      "authors": [
        "Jiabei Cheng",
        "Jiachen Li",
        "Kaiyuan Yang",
        "Hongbin Shen",
        "Ye Yuan"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2411.06331v1",
      "pdf_url": "http://arxiv.org/pdf/2411.06331v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2508.04747v1",
      "source": "arxiv",
      "title": "GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type\n  Annotation",
      "abstract": "Cell type annotation is a fundamental step in the analysis of single-cell RNA\nsequencing (scRNA-seq) data. In practice, human experts often rely on the\nstructure revealed by principal component analysis (PCA) followed by\n$k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While\neffective, this process is labor-intensive and does not scale to large\ndatasets. Recent advances in CLIP-style models offer a promising path toward\nautomating cell type annotation. By aligning scRNA-seq profiles with natural\nlanguage descriptions, models like LangCell enable zero-shot annotation. While\nLangCell demonstrates decent zero-shot performance, its predictions remain\nsuboptimal, particularly in achieving consistent accuracy across all cell\ntypes. In this paper, we propose to refine the zero-shot logits produced by\nLangCell through a graph-regularized optimization framework. By enforcing local\nconsistency over the task-specific PCA-based k-NN graph, our method combines\nthe scalability of the pre-trained models with the structural robustness relied\nupon in expert annotation. We evaluate our approach on 14 annotated human\nscRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000\nsingle cells. Our method consistently improves zero-shot annotation accuracy,\nachieving accuracy gains of up to 10%. Further analysis showcase the mechanism\nby which GRIT effectively propagates correct signals through the graph, pulling\nback mislabeled cells toward more accurate predictions. The method is\ntraining-free, model-agnostic, and serves as a simple yet effective plug-in for\nenhancing automated cell type annotation in practice.",
      "authors": [
        "Tianxiang Hu",
        "Chenyi Zhou",
        "Jiaxiang Liu",
        "Jiongxin Wang",
        "Ruizhe Chen",
        "Haoxiang Xia",
        "Gaoang Wang",
        "Jian Wu",
        "Zuozhu Liu"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2508.04747v1",
      "pdf_url": "http://arxiv.org/pdf/2508.04747v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2505.12638v2",
      "source": "arxiv",
      "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell\n  Chromatin Accessibility Data",
      "abstract": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent ChromFound, a foundation model tailored for scATAC-seq. ChromFound\nutilizes a hybrid architecture and genome-aware tokenization to effectively\ncapture genome-wide long contexts and regulatory signals from dynamic chromatin\nlandscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease\nconditions, ChromFound demonstrates broad applicability across 6 diverse tasks.\nNotably, it achieves robust zero-shot performance in generating universal cell\nrepresentations and exhibits excellent transferability in cell type annotation\nand cross-omics prediction. By uncovering enhancer-gene links undetected by\nexisting computational methods, ChromFound offers a promising framework for\nunderstanding disease risk variants in the noncoding genome.",
      "authors": [
        "Yifeng Jiao",
        "Yuchen Liu",
        "Yu Zhang",
        "Xin Guo",
        "Yushuai Wu",
        "Chen Jiang",
        "Jiyang Li",
        "Hongwei Zhang",
        "Limei Han",
        "Xin Gao",
        "Yuan Qi",
        "Yuan Cheng"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2505.12638v2",
      "pdf_url": "http://arxiv.org/pdf/2505.12638v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2508.15751v1",
      "source": "arxiv",
      "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model",
      "abstract": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.",
      "authors": [
        "Xueyuan Li",
        "Can Cui",
        "Ruining Deng",
        "Yucheng Tang",
        "Quan Liu",
        "Tianyuan Yao",
        "Shunxing Bao",
        "Naweed Chowdhury",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2508.15751v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15751v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2305.04417v1",
      "source": "arxiv",
      "title": "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for\n  Zero-Shot Semantic Annotation of Legal Texts",
      "abstract": "We evaluated the capability of a state-of-the-art generative pre-trained\ntransformer (GPT) model to perform semantic annotation of short text snippets\n(one to few sentences) coming from legal documents of various types.\nDiscussions of potential uses (e.g., document drafting, summarization) of this\nemerging technology in legal domain have intensified, but to date there has not\nbeen a rigorous analysis of these large language models' (LLM) capacity in\nsentence-level semantic annotation of legal texts in zero-shot learning\nsettings. Yet, this particular type of use could unlock many practical\napplications (e.g., in contract review) and research opportunities (e.g., in\nempirical legal studies). We fill the gap with this study. We examined if and\nhow successfully the model can semantically annotate small batches of short\ntext snippets (10-50) based exclusively on concise definitions of the semantic\ntypes. We found that the GPT model performs surprisingly well in zero-shot\nsettings on diverse types of documents (F1=.73 on a task involving court\nopinions, .86 for contracts, and .54 for statutes and regulations). These\nfindings can be leveraged by legal scholars and practicing lawyers alike to\nguide their decisions in integrating LLMs in wide range of workflows involving\nsemantic annotation of legal texts.",
      "authors": [
        "Jaromir Savelka"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2305.04417v1",
      "pdf_url": "http://arxiv.org/pdf/2305.04417v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2304.07991v1",
      "source": "arxiv",
      "title": "One-shot and Partially-Supervised Cell Image Segmentation Using Small\n  Visual Prompt",
      "abstract": "Semantic segmentation of microscopic cell images using deep learning is an\nimportant technique, however, it requires a large number of images and ground\ntruth labels for training. To address the above problem, we consider an\nefficient learning framework with as little data as possible, and we propose\ntwo types of learning strategies: One-shot segmentation which can learn with\nonly one training sample, and Partially-supervised segmentation which assigns\nannotations to only a part of images. Furthermore, we introduce novel\nsegmentation methods using the small prompt images inspired by prompt learning\nin recent studies. Our proposed methods use a pre-trained model based on only\ncell images and teach the information of the prompt pairs to the target image\nto be segmented by the attention mechanism, which allows for efficient learning\nwhile reducing the burden of annotation costs. Through experiments conducted on\nthree types of microscopic cell image datasets, we confirmed that the proposed\nmethod improved the Dice score coefficient (DSC) in comparison with the\nconventional methods.",
      "authors": [
        "Sota Kato",
        "Kazuhiro Hotta"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2304.07991v1",
      "pdf_url": "http://arxiv.org/pdf/2304.07991v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2007.01671v1",
      "source": "arxiv",
      "title": "Few-Shot Microscopy Image Cell Segmentation",
      "abstract": "Automatic cell segmentation in microscopy images works well with the support\nof deep neural networks trained with full supervision. Collecting and\nannotating images, though, is not a sustainable solution for every new\nmicroscopy database and cell type. Instead, we assume that we can access a\nplethora of annotated image data sets from different domains (sources) and a\nlimited number of annotated image data sets from the domain of interest\n(target), where each domain denotes not only different image appearance but\nalso a different type of cell segmentation problem. We pose this problem as\nmeta-learning where the goal is to learn a generic and adaptable few-shot\nlearning model from the available source domain data sets and cell segmentation\ntasks. The model can be afterwards fine-tuned on the few annotated images of\nthe target domain that contains different image appearance and different cell\ntype. In our meta-learning training, we propose the combination of three\nobjective functions to segment the cells, move the segmentation results away\nfrom the classification boundary using cross-domain tasks, and learn an\ninvariant representation between tasks of the source domains. Our experiments\non five public databases show promising results from 1- to 10-shot\nmeta-learning using standard segmentation neural network architectures.",
      "authors": [
        "Youssef Dawoud",
        "Julia Hornauer",
        "Gustavo Carneiro",
        "Vasileios Belagiannis"
      ],
      "year": 2020,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2007.01671v1",
      "pdf_url": "http://arxiv.org/pdf/2007.01671v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2503.08603v1",
      "source": "arxiv",
      "title": "CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer",
      "abstract": "Cell microscopy data are abundant; however, corresponding segmentation\nannotations remain scarce. Moreover, variations in cell types, imaging devices,\nand staining techniques introduce significant domain gaps between datasets. As\na result, even large, pretrained segmentation models trained on diverse\ndatasets (source datasets) struggle to generalize to unseen datasets (target\ndatasets). To overcome this generalization problem, we propose CellStyle, which\nimproves the segmentation quality of such models without requiring labels for\nthe target dataset, thereby enabling zero-shot adaptation. CellStyle transfers\nthe attributes of an unannotated target dataset, such as texture, color, and\nnoise, to the annotated source dataset. This transfer is performed while\npreserving the cell shapes of the source images, ensuring that the existing\nsource annotations can still be used while maintaining the visual\ncharacteristics of the target dataset. The styled synthetic images with the\nexisting annotations enable the finetuning of a generalist segmentation model\nfor application to the unannotated target data. We demonstrate that CellStyle\nsignificantly improves zero-shot cell segmentation performance across diverse\ndatasets by finetuning multiple segmentation models on the style-transferred\ndata. The code will be made publicly available.",
      "authors": [
        "RÃ¼veyda Yilmaz",
        "Zhu Chen",
        "Yuli Wu",
        "Johannes Stegmaier"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2503.08603v1",
      "pdf_url": "http://arxiv.org/pdf/2503.08603v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2503.18170v1",
      "source": "arxiv",
      "title": "Self-Attention Diffusion Models for Zero-Shot Biomedical Image\n  Segmentation: Unlocking New Frontiers in Medical Imaging",
      "abstract": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies.",
      "authors": [
        "Abderrachid Hamrani",
        "Anuradha Godavarty"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2503.18170v1",
      "pdf_url": "http://arxiv.org/pdf/2503.18170v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2502.18702v1",
      "source": "arxiv",
      "title": "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity\n  Recognition",
      "abstract": "Zero-shot named entity recognition (NER) aims to develop entity recognition\nsystems from unannotated text corpora. This task presents substantial\nchallenges due to minimal human intervention. Recent work has adapted large\nlanguage models (LLMs) for zero-shot NER by crafting specialized prompt\ntemplates. It advances model self-learning abilities by incorporating\nself-annotated demonstrations. However, two important challenges persist: (i)\nCorrelations between contexts surrounding entities are overlooked, leading to\nwrong type predictions or entity omissions. (ii) The indiscriminate use of task\ndemonstrations, retrieved through shallow similarity-based strategies, severely\nmisleads LLMs during inference.\n  In this paper, we introduce the cooperative multi-agent system (CMAS), a\nnovel framework for zero-shot NER that uses the collective intelligence of\nmultiple agents to address the challenges outlined above. CMAS has four main\nagents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor,\n(iii) a demonstration discriminator, and (iv) an overall predictor. To\nexplicitly capture correlations between contexts surrounding entities, CMAS\nreformulates NER into two subtasks: recognizing named entities and identifying\nentity type-related features within the target sentence. To enable controllable\nutilization of demonstrations, a demonstration discriminator is established to\nincorporate the self-reflection mechanism, automatically evaluating helpfulness\nscores for the target sentence. Experimental results show that CMAS\nsignificantly improves zero-shot NER performance across six benchmarks,\nincluding both domain-specific and general-domain scenarios. Furthermore, CMAS\ndemonstrates its effectiveness in few-shot settings and with various LLM\nbackbones.",
      "authors": [
        "Zihan Wang",
        "Ziqi Zhao",
        "Yougang Lyu",
        "Zhumin Chen",
        "Maarten de Rijke",
        "Zhaochun Ren"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2502.18702v1",
      "pdf_url": "http://arxiv.org/pdf/2502.18702v1",
      "citations_count": null,
      "keywords": []
    }
  ],
  "reviews": [
    {
      "paper_id": "http://arxiv.org/abs/2411.06331v1",
      "summary": {
        "tldr": "The remarkable success of foundation models has sparked growing interest in\ntheir application to single-cell biology. Models like Geneformer and scGPT\npromise to serve as versatile tools in this specialized field. However,\nrepresenting a cell as a sequence of genes remains an ope...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "The remarkable success of foundation models has sparked growing interest in\ntheir application to single-cell biology. Models like Geneformer and scGPT\npromise to serve as versatile tools in this speci",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2508.04747v1",
      "summary": {
        "tldr": "Cell type annotation is a fundamental step in the analysis of single-cell RNA\nsequencing (scRNA-seq) data. In practice, human experts often rely on the\nstructure revealed by principal component analysis (PCA) followed by\n$k$-nearest neighbor ($k$-NN) graph construction to guide a...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Cell type annotation is a fundamental step in the analysis of single-cell RNA\nsequencing (scRNA-seq) data. In practice, human experts often rely on the\nstructure revealed by principal component analys",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2505.12638v2",
      "summary": {
        "tldr": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achi...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repositor",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2508.15751v1",
      "summary": {
        "tldr": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentat",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2305.04417v1",
      "summary": {
        "tldr": "We evaluated the capability of a state-of-the-art generative pre-trained\ntransformer (GPT) model to perform semantic annotation of short text snippets\n(one to few sentences) coming from legal documents of various types.\nDiscussions of potential uses (e.g., document drafting, summ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "We evaluated the capability of a state-of-the-art generative pre-trained\ntransformer (GPT) model to perform semantic annotation of short text snippets\n(one to few sentences) coming from legal document",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "overclaiming",
            "severity": "medium",
            "rationale": "Claims strong superiority; check against baselines in paper."
          },
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2304.07991v1",
      "summary": {
        "tldr": "Semantic segmentation of microscopic cell images using deep learning is an\nimportant technique, however, it requires a large number of images and ground\ntruth labels for training. To address the above problem, we consider an\nefficient learning framework with as little data as pos...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Semantic segmentation of microscopic cell images using deep learning is an\nimportant technique, however, it requires a large number of images and ground\ntruth labels for training. To address the above",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2007.01671v1",
      "summary": {
        "tldr": "Automatic cell segmentation in microscopy images works well with the support\nof deep neural networks trained with full supervision. Collecting and\nannotating images, though, is not a sustainable solution for every new\nmicroscopy database and cell type. Instead, we assume that we ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Automatic cell segmentation in microscopy images works well with the support\nof deep neural networks trained with full supervision. Collecting and\nannotating images, though, is not a sustainable solut",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2503.08603v1",
      "summary": {
        "tldr": "Cell microscopy data are abundant; however, corresponding segmentation\nannotations remain scarce. Moreover, variations in cell types, imaging devices,\nand staining techniques introduce significant domain gaps between datasets. As\na result, even large, pretrained segmentation mode...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Cell microscopy data are abundant; however, corresponding segmentation\nannotations remain scarce. Moreover, variations in cell types, imaging devices,\nand staining techniques introduce significant dom",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2503.18170v1",
      "summary": {
        "tldr": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilit...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentatio",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2502.18702v1",
      "summary": {
        "tldr": "Zero-shot named entity recognition (NER) aims to develop entity recognition\nsystems from unannotated text corpora. This task presents substantial\nchallenges due to minimal human intervention. Recent work has adapted large\nlanguage models (LLMs) for zero-shot NER by crafting speci...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Zero-shot named entity recognition (NER) aims to develop entity recognition\nsystems from unannotated text corpora. This task presents substantial\nchallenges due to minimal human intervention",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    }
  ],
  "matrix": {
    "columns": [
      "Venue",
      "Year",
      "Citations",
      "Methods",
      "Results",
      "Limitations"
    ],
    "rows": [
      "GenoHoption: Bridging Gene Network Graphs and Single-Cell Foundation\n  Models",
      "GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type\n  Annotation",
      "ChromFound: Towards A Universal Foundation Model for Single-Cell\n  Chromatin Accessibility Data",
      "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model",
      "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for\n  Zero-Shot Semantic Annotation of Legal Texts",
      "One-shot and Partially-Supervised Cell Image Segmentation Using Small\n  Visual Prompt",
      "Few-Shot Microscopy Image Cell Segmentation",
      "CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer",
      "Self-Attention Diffusion Models for Zero-Shot Biomedical Image\n  Segmentation: Unlocking New Frontiers in Medical Imaging",
      "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity\n  Recognition"
    ],
    "data": {
      "GenoHoption: Bridging Gene Network Graphs and Single-Cell Foundation\n  Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type\n  Annotation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "ChromFound: Towards A Universal Foundation Model for Single-Cell\n  Chromatin Accessibility Data": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for\n  Zero-Shot Semantic Annotation of Legal Texts": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "One-shot and Partially-Supervised Cell Image Segmentation Using Small\n  Visual Prompt": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Few-Shot Microscopy Image Cell Segmentation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2020"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Self-Attention Diffusion Models for Zero-Shot Biomedical Image\n  Segmentation: Unlocking New Frontiers in Medical Imaging": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity\n  Recognition": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      }
    }
  },
  "synthesis": {
    "executive_summary": "Reviewed 10 papers across 1 venues. We summarize methods, results, and limitations, and identify common gaps.",
    "gaps": [
      "Few works report comparisons against strong baselines."
    ],
    "future_work": [
      "Open-sourcing code and datasets",
      "Larger, diverse cohorts",
      "Robust baseline comparisons"
    ]
  },
  "artifacts": {
    "markdown_path": "outputs\\review_zero-shot-cell-type-annotation-with-geneformer-model.md",
    "json_path": "outputs\\review_zero-shot-cell-type-annotation-with-geneformer-model.json",
    "csv_path": "outputs\\papers_zero-shot-cell-type-annotation-with-geneformer-model.csv"
  },
  "created_at": "2025-09-13T06:22:55.487905"
}