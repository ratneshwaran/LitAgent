{
  "topic": "zeroshot",
  "filters": {
    "start_year": null,
    "end_year": null,
    "include_keywords": [],
    "exclude_keywords": [],
    "venues": [],
    "limit": 10
  },
  "raw_papers": [
    {
      "id": "http://arxiv.org/abs/2505.21239v1",
      "source": "arxiv",
      "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners",
      "abstract": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD",
      "authors": [
        "Yu He",
        "Zihan Yao",
        "Chentao Song",
        "Tianyu Qi",
        "Jun Liu",
        "Ming Li",
        "Qing Huang"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2505.21239v1",
      "pdf_url": "http://arxiv.org/pdf/2505.21239v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2507.02950v2",
      "source": "arxiv",
      "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator\n  Roles Assessed by Motivational Interviewing Criteria",
      "abstract": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools.",
      "authors": [
        "Keita Kiuchi",
        "Yoshikazu Fujimoto",
        "Hideyuki Goto",
        "Tomonori Hosokawa",
        "Makoto Nishimura",
        "Yosuke Sato",
        "Izumi Sezai"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2507.02950v2",
      "pdf_url": "http://arxiv.org/pdf/2507.02950v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2409.13703v1",
      "source": "arxiv",
      "title": "Zeroshot Listwise Learning to Rank Algorithm for Recommendation",
      "abstract": "Learning to rank is a rare technology compared with other techniques such as\ndeep neural networks. The number of experts in the field is roughly 1/6 of the\nnumber of professionals in deep learning. Being an effective ranking\nmethodology, learning to rank has been widely used in the field of information\nretrieval. However, in recent years, learning to rank as a recommendation\napproach has been on decline. In this paper, we take full advantage of order\nstatistic approximation and power law distribution to design a zeroshot\nlistwise learning to rank algorithm for recommendation. We prove in the\nexperiment section that our approach is both accurate and fair.",
      "authors": [
        "Hao Wang"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2409.13703v1",
      "pdf_url": "http://arxiv.org/pdf/2409.13703v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2403.09307v3",
      "source": "arxiv",
      "title": "Annotation Free Semantic Segmentation with Vision Foundation Models",
      "abstract": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zeroshot semantic\nsegmentation while requiring either large-scale training or additional\nimage/pixel level annotations. In this work, we generate free annotations for\nany semantic segmentation dataset using existing foundation models. We use CLIP\nto detect objects and SAM to generate high quality object masks. Next, we build\na lightweight module on top of a self-supervised vision encoder, DinoV2, to\nalign the patch features with a pretrained text encoder for zeroshot semantic\nsegmentation. Our approach can bring language-based semantics to any pretrained\nvision encoder with minimal training, uses foundation models as the sole source\nof supervision and generalizes from little training data with no annotation.",
      "authors": [
        "Soroush Seifi",
        "Daniel Olmeda Reino",
        "Fabien Despinoy",
        "Rahaf Aljundi"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2403.09307v3",
      "pdf_url": "http://arxiv.org/pdf/2403.09307v3",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2403.16194v1",
      "source": "arxiv",
      "title": "Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised\n  Landmark Discovery",
      "abstract": "Unsupervised landmarks discovery (ULD) for an object category is a\nchallenging computer vision problem. In pursuit of developing a robust ULD\nframework, we explore the potential of a recent paradigm of self-supervised\nlearning algorithms, known as diffusion models. Some recent works have shown\nthat these models implicitly contain important correspondence cues. Towards\nharnessing the potential of diffusion models for the ULD task, we make the\nfollowing core contributions. First, we propose a ZeroShot ULD baseline based\non simple clustering of random pixel locations with nearest neighbour matching.\nIt delivers better results than existing ULD methods. Second, motivated by the\nZeroShot performance, we develop a ULD algorithm based on diffusion features\nusing self-training and clustering which also outperforms prior methods by\nnotable margins. Third, we introduce a new proxy task based on generating\nlatent pose codes and also propose a two-stage clustering mechanism to\nfacilitate effective pseudo-labeling, resulting in a significant performance\nimprovement. Overall, our approach consistently outperforms state-of-the-art\nmethods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by\nsignificant margins.",
      "authors": [
        "Siddharth Tourani",
        "Ahmed Alwheibi",
        "Arif Mahmood",
        "Muhammad Haris Khan"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2403.16194v1",
      "pdf_url": "http://arxiv.org/pdf/2403.16194v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2406.07036v1",
      "source": "arxiv",
      "title": "Paying More Attention to Source Context: Mitigating Unfaithful\n  Translations from Large Language Model",
      "abstract": "Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between source and target contexts. Analyzing\ncontribution scores during generation processes revealed that LLMs can be\nbiased towards previously generated tokens over corresponding source tokens,\nleading to unfaithful translations. To address this issue, we propose to\nencourage LLMs to pay more attention to the source context from both source and\ntarget perspectives in zeroshot prompting: 1) adjust source context attention\nweights; 2) suppress irrelevant target prefix influence; Additionally, we\npropose 3) avoiding over-reliance on the target prefix in instruction tuning.\nExperimental results from both human-collected unfaithfulness test sets\nfocusing on LLM-generated unfaithful translations and general test sets, verify\nour methods' effectiveness across multiple language pairs. Further human\nevaluation shows our method's efficacy in reducing hallucinatory translations\nand facilitating faithful translation generation.",
      "authors": [
        "Hongbin Zhang",
        "Kehai Chen",
        "Xuefeng Bai",
        "Yang Xiang",
        "Min Zhang"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2406.07036v1",
      "pdf_url": "http://arxiv.org/pdf/2406.07036v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2406.14830v1",
      "source": "arxiv",
      "title": "CLIP-Decoder : ZeroShot Multilabel Classification using Multimodal CLIP\n  Aligned Representation",
      "abstract": "Multi-label classification is an essential task utilized in a wide variety of\nreal-world applications. Multi-label zero-shot learning is a method for\nclassifying images into multiple unseen categories for which no training data\nis available, while in general zero-shot situations, the test set may include\nobserved classes. The CLIP-Decoder is a novel method based on the\nstate-of-the-art ML-Decoder attention-based head. We introduce multi-modal\nrepresentation learning in CLIP-Decoder, utilizing the text encoder to extract\ntext features and the image encoder for image feature extraction. Furthermore,\nwe minimize semantic mismatch by aligning image and word embeddings in the same\ndimension and comparing their respective representations using a combined loss,\nwhich comprises classification loss and CLIP loss. This strategy outperforms\nother methods and we achieve cutting-edge results on zero-shot multilabel\nclassification tasks using CLIP-Decoder. Our method achieves an absolute\nincrease of 3.9% in performance compared to existing methods for zero-shot\nlearning multi-label classification tasks. Additionally, in the generalized\nzero-shot learning multi-label classification task, our method shows an\nimpressive increase of almost 2.3%.",
      "authors": [
        "Muhammad Ali",
        "Salman Khan"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2406.14830v1",
      "pdf_url": "http://arxiv.org/pdf/2406.14830v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2312.17543v2",
      "source": "arxiv",
      "title": "Building Efficient Universal Classifiers with Natural Language Inference",
      "abstract": "Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation.\nMany users, however, do not need the broad capabilities of generative LLMs when\nthey only want to automate a classification task. Smaller BERT-like models can\nalso learn universal tasks, which allow them to do any text classification task\nwithout requiring fine-tuning (zeroshot classification) or to learn new tasks\nwith only a few examples (fewshot), while being significantly more efficient\nthan generative LLMs. This paper (1) explains how Natural Language Inference\n(NLI) can be used as a universal classification task that follows similar\nprinciples as instruction fine-tuning of generative LLMs, (2) provides a\nstep-by-step guide with reusable Jupyter notebooks for building a universal\nclassifier, and (3) shares the resulting universal classifier that is trained\non 33 datasets with 389 diverse classes. Parts of the code we share has been\nused to train our older zeroshot classifiers that have been downloaded more\nthan 55 million times via the Hugging Face Hub as of December 2023. Our new\nclassifier improves zeroshot performance by 9.4%.",
      "authors": [
        "Moritz Laurer",
        "Wouter van Atteveldt",
        "Andreu Casas",
        "Kasper Welbers"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2312.17543v2",
      "pdf_url": "http://arxiv.org/pdf/2312.17543v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2305.13083v1",
      "source": "arxiv",
      "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling\n  from GPT",
      "abstract": "While large models such as GPT-3 demonstrate exceptional performance in\nzeroshot and fewshot summarization tasks, their extensive serving and\nfine-tuning costs hinder their utilization in various applications. Conversely,\nprevious studies have found that although automatic metrics tend to favor\nsmaller fine-tuned models, the quality of the summaries they generate is\ninferior to that of larger models like GPT-3 when assessed by human evaluators.\nTo address this issue, we propose InheritSumm, a versatile and compact\nsummarization model derived from GPT-3.5 through distillation. InheritSumm not\nonly exhibits comparable zeroshot and fewshot summarization capabilities to\nGPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental\nresults demonstrate that InheritSumm achieves similar or superior performance\nto GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the\npreviously established best small models in both prefix-tuning and full-data\nfine-tuning scenarios.",
      "authors": [
        "Yichong Xu",
        "Ruochen Xu",
        "Dan Iter",
        "Yang Liu",
        "Shuohang Wang",
        "Chenguang Zhu",
        "Michael Zeng"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2305.13083v1",
      "pdf_url": "http://arxiv.org/pdf/2305.13083v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2307.05680v1",
      "source": "arxiv",
      "title": "LogitMat : Zeroshot Learning Algorithm for Recommender Systems without\n  Transfer Learning or Pretrained Models",
      "abstract": "Recommender system is adored in the internet industry as one of the most\nprofitable technologies. Unlike other sectors such as fraud detection in the\nFintech industry, recommender system is both deep and broad. In recent years,\nmany researchers start to focus on the cold-start problem of recommender\nsystems. In spite of the large volume of research literature, the majority of\nthe research utilizes transfer learning / meta learning and pretrained model to\nsolve the problem. Although the researchers claim the effectiveness of the\napproaches, everyone of them does rely on extra input data from other sources.\nIn 2021 and 2022, several zeroshot learning algorithm for recommender system\nsuch as ZeroMat, DotMat, PoissonMat and PowerMat were invented. They are the\nfirst batch of the algorithms that rely on no transfer learning or pretrained\nmodels to tackle the problem. In this paper, we follow this line and invent a\nnew zeroshot learning algorithm named LogitMat. We take advantage of the Zipf\nLaw property of the user item rating values and logistic regression model to\ntackle the cold-start problem and generate competitive results with other\ncompeting techniques. We prove in experiments that our algorithm is fast,\nrobust and effective.",
      "authors": [
        "Hao Wang"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2307.05680v1",
      "pdf_url": "http://arxiv.org/pdf/2307.05680v1",
      "citations_count": null,
      "keywords": []
    }
  ],
  "reviews": [
    {
      "paper_id": "http://arxiv.org/abs/2505.21239v1",
      "summary": {
        "tldr": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interactio...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2507.02950v2",
      "summary": {
        "tldr": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed cou",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "overclaiming",
            "severity": "medium",
            "rationale": "Claims strong superiority; check against baselines in paper."
          },
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2409.13703v1",
      "summary": {
        "tldr": "Learning to rank is a rare technology compared with other techniques such as\ndeep neural networks. The number of experts in the field is roughly 1/6 of the\nnumber of professionals in deep learning. Being an effective ranking\nmethodology, learning to rank has been widely used in t...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Learning to rank is a rare technology compared with other techniques such as\ndeep neural networks. The number of experts in the field is roughly 1/6 of the\nnumber of professionals in deep learning",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2403.09307v3",
      "summary": {
        "tldr": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zeroshot semant...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and es",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2403.16194v1",
      "summary": {
        "tldr": "Unsupervised landmarks discovery (ULD) for an object category is a\nchallenging computer vision problem. In pursuit of developing a robust ULD\nframework, we explore the potential of a recent paradigm of self-supervised\nlearning algorithms, known as diffusion models. Some recent wo...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Unsupervised landmarks discovery (ULD) for an object category is a\nchallenging computer vision problem. In pursuit of developing a robust ULD\nframework, we explore the potential of a recent paradigm o",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2406.07036v1",
      "summary": {
        "tldr": "Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between source and target contexts. Analyzing\ncontribution scores during generation processe...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between sour",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2406.14830v1",
      "summary": {
        "tldr": "Multi-label classification is an essential task utilized in a wide variety of\nreal-world applications. Multi-label zero-shot learning is a method for\nclassifying images into multiple unseen categories for which no training data\nis available, while in general zero-shot situations,...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Multi-label classification is an essential task utilized in a wide variety of\nreal-world applications. Multi-label zero-shot learning is a method for\nclassifying images into multiple unseen categories",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2312.17543v2",
      "summary": {
        "tldr": "Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation.\nMany users, however, do not need the broad capabilities of generative LLMs when\nthey only want to automate a classification t...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad ca",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2305.13083v1",
      "summary": {
        "tldr": "While large models such as GPT-3 demonstrate exceptional performance in\nzeroshot and fewshot summarization tasks, their extensive serving and\nfine-tuning costs hinder their utilization in various applications. Conversely,\nprevious studies have found that although automatic metric...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "While large models such as GPT-3 demonstrate exceptional performance in\nzeroshot and fewshot summarization tasks, their extensive serving and\nfine-tuning costs hinder their utilization in various appl",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2307.05680v1",
      "summary": {
        "tldr": "Recommender system is adored in the internet industry as one of the most\nprofitable technologies. Unlike other sectors such as fraud detection in the\nFintech industry, recommender system is both deep and broad. In recent years,\nmany researchers start to focus on the cold-start pr...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Recommender system is adored in the internet industry as one of the most\nprofitable technologies. Unlike other sectors such as fraud detection in the\nFintech industry, recommender system is both deep ",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    }
  ],
  "matrix": {
    "columns": [
      "Venue",
      "Year",
      "Citations",
      "Methods",
      "Results",
      "Limitations"
    ],
    "rows": [
      "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners",
      "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator\n  Roles Assessed by Motivational Interviewing Criteria",
      "Zeroshot Listwise Learning to Rank Algorithm for Recommendation",
      "Annotation Free Semantic Segmentation with Vision Foundation Models",
      "Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised\n  Landmark Discovery",
      "Paying More Attention to Source Context: Mitigating Unfaithful\n  Translations from Large Language Model",
      "CLIP-Decoder : ZeroShot Multilabel Classification using Multimodal CLIP\n  Aligned Representation",
      "Building Efficient Universal Classifiers with Natural Language Inference",
      "InheritSumm: A General, Versatile and Compact Summarizer by Distilling\n  from GPT",
      "LogitMat : Zeroshot Learning Algorithm for Recommender Systems without\n  Transfer Learning or Pretrained Models"
    ],
    "data": {
      "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator\n  Roles Assessed by Motivational Interviewing Criteria": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Zeroshot Listwise Learning to Rank Algorithm for Recommendation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Annotation Free Semantic Segmentation with Vision Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised\n  Landmark Discovery": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Paying More Attention to Source Context: Mitigating Unfaithful\n  Translations from Large Language Model": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "CLIP-Decoder : ZeroShot Multilabel Classification using Multimodal CLIP\n  Aligned Representation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Building Efficient Universal Classifiers with Natural Language Inference": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "InheritSumm: A General, Versatile and Compact Summarizer by Distilling\n  from GPT": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "LogitMat : Zeroshot Learning Algorithm for Recommender Systems without\n  Transfer Learning or Pretrained Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      }
    }
  },
  "synthesis": {
    "executive_summary": "Reviewed 10 papers across 1 venues. We summarize methods, results, and limitations, and identify common gaps.",
    "gaps": [
      "Few works report comparisons against strong baselines."
    ],
    "future_work": [
      "Open-sourcing code and datasets",
      "Larger, diverse cohorts",
      "Robust baseline comparisons"
    ]
  },
  "artifacts": {
    "markdown_path": "outputs\\review_zeroshot.md",
    "json_path": "outputs\\review_zeroshot.json",
    "csv_path": "outputs\\papers_zeroshot.csv"
  },
  "created_at": "2025-09-13T06:19:15.450609"
}