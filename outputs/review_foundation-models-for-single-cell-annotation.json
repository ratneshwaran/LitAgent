{
  "topic": "foundation models for single-cell annotation",
  "filters": {
    "start_year": null,
    "end_year": null,
    "include_keywords": [],
    "exclude_keywords": [],
    "venues": [],
    "limit": 25
  },
  "raw_papers": [
    {
      "id": "http://arxiv.org/abs/2507.02743v1",
      "source": "arxiv",
      "title": "Prompt learning with bounding box constraints for medical image\n  segmentation",
      "abstract": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git",
      "authors": [
        "Mélanie Gaillochet",
        "Mehrdad Noori",
        "Sahar Dastani",
        "Christian Desrosiers",
        "Hervé Lombaert"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2507.02743v1",
      "pdf_url": "http://arxiv.org/pdf/2507.02743v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2502.03907v1",
      "source": "arxiv",
      "title": "No Free Lunch in Annotation either: An objective evaluation of\n  foundation models for streamlining annotation in animal tracking",
      "abstract": "We analyze the capabilities of foundation models addressing the tedious task\nof generating annotations for animal tracking. Annotating a large amount of\ndata is vital and can be a make-or-break factor for the robustness of a\ntracking model. Robustness is particularly crucial in animal tracking, as\naccurate tracking over long time horizons is essential for capturing the\nbehavior of animals. However, generating additional annotations using\nfoundation models can be counterproductive, as the quality of the annotations\nis just as important. Poorly annotated data can introduce noise and\ninaccuracies, ultimately compromising the performance and accuracy of the\ntrained model. Over-reliance on automated annotations without ensuring\nprecision can lead to diminished results, making careful oversight and quality\ncontrol essential in the annotation process. Ultimately, we demonstrate that a\nthoughtful combination of automated annotations and manually annotated data is\na valuable strategy, yielding an IDF1 score of 80.8 against blind usage of SAM2\nvideo with an IDF1 score of 65.6.",
      "authors": [
        "Emil Mededovic",
        "Valdy Laurentius",
        "Yuli Wu",
        "Marcin Kopaczka",
        "Zhu Chen",
        "Mareike Schulz",
        "René Tolba",
        "Johannes Stegmaier"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2502.03907v1",
      "pdf_url": "http://arxiv.org/pdf/2502.03907v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2503.09860v1",
      "source": "arxiv",
      "title": "Foundation X: Integrating Classification, Localization, and Segmentation\n  through Lock-Release Pretraining Strategy for Chest X-ray Analysis",
      "abstract": "Developing robust and versatile deep-learning models is essential for\nenhancing diagnostic accuracy and guiding clinical interventions in medical\nimaging, but it requires a large amount of annotated data. The advancement of\ndeep learning has facilitated the creation of numerous medical datasets with\ndiverse expert-level annotations. Aggregating these datasets can maximize data\nutilization and address the inadequacy of labeled data. However, the\nheterogeneity of expert-level annotations across tasks such as classification,\nlocalization, and segmentation presents a significant challenge for learning\nfrom these datasets. To this end, we introduce nFoundation X, an end-to-end\nframework that utilizes diverse expert-level annotations from numerous public\ndatasets to train a foundation model capable of multiple tasks including\nclassification, localization, and segmentation. To address the challenges of\nannotation and task heterogeneity, we propose a Lock-Release pretraining\nstrategy to enhance the cyclic learning from multiple datasets, combined with\nthe student-teacher learning paradigm, ensuring the model retains general\nknowledge for all tasks while preventing overfitting to any single task. To\ndemonstrate the effectiveness of Foundation X, we trained a model using 11\nchest X-ray datasets, covering annotations for classification, localization,\nand segmentation tasks. Our experimental results show that Foundation X\nachieves notable performance gains through extensive annotation utilization,\nexcels in cross-dataset and cross-task learning, and further enhances\nperformance in organ localization and segmentation tasks. All code and\npretrained models are publicly accessible at\nhttps://github.com/jlianglab/Foundation_X.",
      "authors": [
        "Nahid Ul Islam",
        "DongAo Ma",
        "Jiaxuan Pang",
        "Shivasakthi Senthil Velan",
        "Michael Gotway",
        "Jianming Liang"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2503.09860v1",
      "pdf_url": "http://arxiv.org/pdf/2503.09860v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2506.02914v1",
      "source": "arxiv",
      "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through\n  3D LiDAR Detection",
      "abstract": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark",
      "authors": [
        "Yechi Ma",
        "Wei Hua",
        "Shu Kong"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2506.02914v1",
      "pdf_url": "http://arxiv.org/pdf/2506.02914v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2508.15751v1",
      "source": "arxiv",
      "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model",
      "abstract": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.",
      "authors": [
        "Xueyuan Li",
        "Can Cui",
        "Ruining Deng",
        "Yucheng Tang",
        "Quan Liu",
        "Tianyuan Yao",
        "Shunxing Bao",
        "Naweed Chowdhury",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2508.15751v1",
      "pdf_url": "http://arxiv.org/pdf/2508.15751v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40537825",
      "source": "pubmed",
      "title": "scExtract: leveraging large language models for fully automated single-cell RNA-seq data annotation and prior-informed multi-dataset integration.",
      "abstract": null,
      "authors": [
        "Wu Y",
        "Tang F"
      ],
      "year": 2025,
      "venue": "Genome biology",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40537825/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2403.09307v3",
      "source": "arxiv",
      "title": "Annotation Free Semantic Segmentation with Vision Foundation Models",
      "abstract": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zeroshot semantic\nsegmentation while requiring either large-scale training or additional\nimage/pixel level annotations. In this work, we generate free annotations for\nany semantic segmentation dataset using existing foundation models. We use CLIP\nto detect objects and SAM to generate high quality object masks. Next, we build\na lightweight module on top of a self-supervised vision encoder, DinoV2, to\nalign the patch features with a pretrained text encoder for zeroshot semantic\nsegmentation. Our approach can bring language-based semantics to any pretrained\nvision encoder with minimal training, uses foundation models as the sole source\nof supervision and generalizes from little training data with no annotation.",
      "authors": [
        "Soroush Seifi",
        "Daniel Olmeda Reino",
        "Fabien Despinoy",
        "Rahaf Aljundi"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2403.09307v3",
      "pdf_url": "http://arxiv.org/pdf/2403.09307v3",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2403.15218v1",
      "source": "arxiv",
      "title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations",
      "abstract": "Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).",
      "authors": [
        "Pranav Kulkarni",
        "Adway Kanhere",
        "Dharmam Savani",
        "Andrew Chan",
        "Devina Chatterjee",
        "Paul H. Yi",
        "Vishwa S. Parekh"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2403.15218v1",
      "pdf_url": "http://arxiv.org/pdf/2403.15218v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2403.07687v1",
      "source": "arxiv",
      "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost",
      "abstract": "Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.",
      "authors": [
        "Oana Ignat",
        "Longju Bai",
        "Joan Nwatu",
        "Rada Mihalcea"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2403.07687v1",
      "pdf_url": "http://arxiv.org/pdf/2403.07687v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2401.04464v2",
      "source": "arxiv",
      "title": "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
      "abstract": "Massive amounts of unlabelled data are captured by Earth Observation (EO)\nsatellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.\nThis makes Remote Sensing a data-rich domain well suited to Machine Learning\n(ML) solutions. However, a bottleneck in applying ML models to EO is the lack\nof annotated data as annotation is a labour-intensive and costly process. As a\nresult, research in this domain has focused on Self-Supervised Learning and\nFoundation Model approaches. This paper addresses the need to evaluate\ndifferent Foundation Models on a fair and uniform benchmark by introducing the\nPhilEO Bench, a novel evaluation framework for EO Foundation Models. The\nframework comprises of a testbed and a novel 400 GB Sentinel-2 dataset\ncontaining labels for three downstream tasks, building density estimation, road\nsegmentation, and land cover classification. We present experiments using our\nframework evaluating different Foundation Models, including Prithvi and SatMAE,\nat multiple n-shots and convergence rates.",
      "authors": [
        "Casper Fibaek",
        "Luke Camilleri",
        "Andreas Luyts",
        "Nikolaos Dionelis",
        "Bertrand Le Saux"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2401.04464v2",
      "pdf_url": "http://arxiv.org/pdf/2401.04464v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2406.18239v1",
      "source": "arxiv",
      "title": "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets",
      "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
      "authors": [
        "Simon Münker",
        "Kai Kugler",
        "Achim Rettinger"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2406.18239v1",
      "pdf_url": "http://arxiv.org/pdf/2406.18239v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2402.06560v1",
      "source": "arxiv",
      "title": "Video Annotator: A framework for efficiently building video classifiers\n  using vision-language models and active learning",
      "abstract": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain experts. Hard samples, which are\nusually the most informative for model training, tend to be difficult to label\naccurately and consistently without business context. These can arise\nunpredictably during the annotation process, requiring a variable number of\niterations and rounds of feedback, leading to unforeseen expenses and time\ncommitments to guarantee quality.\n  We posit that more direct involvement of domain experts, using a\nhuman-in-the-loop system, can resolve many of these practical challenges. We\npropose a novel framework we call Video Annotator (VA) for annotating,\nmanaging, and iterating on video classification datasets. Our approach offers a\nnew paradigm for an end-user-centered model development process, enhancing the\nefficiency, usability, and effectiveness of video classifiers. Uniquely, VA\nallows for a continuous annotation process, seamlessly integrating data\ncollection and model training.\n  We leverage the zero-shot capabilities of vision-language foundation models\ncombined with active learning techniques, and demonstrate that VA enables the\nefficient creation of high-quality models. VA achieves a median 6.8 point\nimprovement in Average Precision relative to the most competitive baseline\nacross a wide-ranging assortment of tasks. We release a dataset with 153k\nlabels across 56 video understanding tasks annotated by three professional\nvideo editors using VA, and also release code to replicate our experiments at:\nhttp://github.com/netflix/videoannotator.",
      "authors": [
        "Amir Ziai",
        "Aneesh Vartakavi"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2402.06560v1",
      "pdf_url": "http://arxiv.org/pdf/2402.06560v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2402.08075v1",
      "source": "arxiv",
      "title": "Efficient and Scalable Fine-Tune of Language Models for Genome\n  Understanding",
      "abstract": "Although DNA foundation models have advanced the understanding of genomes,\nthey still face significant challenges in the limited scale and diversity of\ngenomic data. This limitation starkly contrasts with the success of natural\nlanguage foundation models, which thrive on substantially larger scales.\nFurthermore, genome understanding involves numerous downstream genome\nannotation tasks with inherent data heterogeneity, thereby necessitating more\nefficient and robust fine-tuning methods tailored for genomics. Here, we\npresent \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for\n\\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo}\nstrategically leverages natural language foundation models' contextual cues,\nrecalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo}\nfurther accommodates numerous, heterogeneous downstream fine-tune tasks by an\nadaptive rank sampling method that prunes and stochastically reintroduces\npruned singular vectors within small computational budgets. Adaptive rank\nsampling outperformed existing fine-tuning methods on all benchmarked 14 genome\nunderstanding tasks, while requiring fewer than 2\\% of trainable parameters as\ngenomic-specific adapters. Impressively, applying these adapters on natural\nlanguage foundation models matched or even exceeded the performance of DNA\nfoundation models. \\textsc{Lingo} presents a new paradigm of efficient and\nscalable genome understanding via genomic-specific adapters on language models.",
      "authors": [
        "Huixin Zhan",
        "Ying Nian Wu",
        "Zijun Zhang"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2402.08075v1",
      "pdf_url": "http://arxiv.org/pdf/2402.08075v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2408.05618v1",
      "source": "arxiv",
      "title": "UrFound: Towards Universal Retinal Foundation Models via\n  Knowledge-Guided Masked Modeling",
      "abstract": "Retinal foundation models aim to learn generalizable representations from\ndiverse retinal images, facilitating label-efficient model adaptation across\nvarious ophthalmic tasks. Despite their success, current retinal foundation\nmodels are generally restricted to a single imaging modality, such as Color\nFundus Photography (CFP) or Optical Coherence Tomography (OCT), limiting their\nversatility. Moreover, these models may struggle to fully leverage expert\nannotations and overlook the valuable domain knowledge essential for\ndomain-specific representation learning. To overcome these limitations, we\nintroduce UrFound, a retinal foundation model designed to learn universal\nrepresentations from both multimodal retinal images and domain knowledge.\nUrFound is equipped with a modality-agnostic image encoder and accepts either\nCFP or OCT images as inputs. To integrate domain knowledge into representation\nlearning, we encode expert annotation in text supervision and propose a\nknowledge-guided masked modeling strategy for model pre-training. It involves\nreconstructing randomly masked patches of retinal images while predicting\nmasked text tokens conditioned on the corresponding retinal image. This\napproach aligns multimodal images and textual expert annotations within a\nunified latent space, facilitating generalizable and domain-specific\nrepresentation learning. Experimental results demonstrate that UrFound exhibits\nstrong generalization ability and data efficiency when adapting to various\ntasks in retinal image analysis. By training on ~180k retinal images, UrFound\nsignificantly outperforms the state-of-the-art retinal foundation model trained\non up to 1.6 million unlabelled images across 8 public retinal datasets. Our\ncode and data are available at https://github.com/yukkai/UrFound.",
      "authors": [
        "Kai Yu",
        "Yang Zhou",
        "Yang Bai",
        "Zhi Da Soh",
        "Xinxing Xu",
        "Rick Siow Mong Goh",
        "Ching-Yu Cheng",
        "Yong Liu"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2408.05618v1",
      "pdf_url": "http://arxiv.org/pdf/2408.05618v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2309.08108v1",
      "source": "arxiv",
      "title": "Foundation Model Assisted Automatic Speech Emotion Recognition:\n  Transcribing, Annotating, and Augmenting",
      "abstract": "Significant advances are being made in speech emotion recognition (SER) using\ndeep learning models. Nonetheless, training SER systems remains challenging,\nrequiring both time and costly resources. Like many other machine learning\ntasks, acquiring datasets for SER requires substantial data annotation efforts,\nincluding transcription and labeling. These annotation processes present\nchallenges when attempting to scale up conventional SER systems. Recent\ndevelopments in foundational models have had a tremendous impact, giving rise\nto applications such as ChatGPT. These models have enhanced human-computer\ninteractions including bringing unique possibilities for streamlining data\ncollection in fields like SER. In this research, we explore the use of\nfoundational models to assist in automating SER from transcription and\nannotation to augmentation. Our study demonstrates that these models can\ngenerate transcriptions to enhance the performance of SER systems that rely\nsolely on speech data. Furthermore, we note that annotating emotions from\ntranscribed speech remains a challenging task. However, combining outputs from\nmultiple LLMs enhances the quality of annotations. Lastly, our findings suggest\nthe feasibility of augmenting existing speech emotion datasets by annotating\nunlabeled speech samples.",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2309.08108v1",
      "pdf_url": "http://arxiv.org/pdf/2309.08108v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2311.08077v2",
      "source": "arxiv",
      "title": "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model\n  (SAM)",
      "abstract": "The advent of foundation models signals a new era in artificial intelligence.\nThe Segment Anything Model (SAM) is the first foundation model for image\nsegmentation. In this study, we evaluate SAM's ability to segment features from\neye images recorded in virtual reality setups. The increasing requirement for\nannotated eye-image datasets presents a significant opportunity for SAM to\nredefine the landscape of data annotation in gaze estimation. Our investigation\ncenters on SAM's zero-shot learning abilities and the effectiveness of prompts\nlike bounding boxes or point clicks. Our results are consistent with studies in\nother domains, demonstrating that SAM's segmentation effectiveness can be\non-par with specialized models depending on the feature, with prompts improving\nits performance, evidenced by an IoU of 93.34% for pupil segmentation in one\ndataset. Foundation models like SAM could revolutionize gaze estimation by\nenabling quick and easy image segmentation, reducing reliance on specialized\nmodels and extensive manual annotation.",
      "authors": [
        "Virmarie Maquiling",
        "Sean Anthony Byrne",
        "Diederick C. Niehorster",
        "Marcus Nyström",
        "Enkelejda Kasneci"
      ],
      "year": 2023,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2311.08077v2",
      "pdf_url": "http://arxiv.org/pdf/2311.08077v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2112.07475v2",
      "source": "arxiv",
      "title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks",
      "abstract": "Labelled data is the foundation of most natural language processing tasks.\nHowever, labelling data is difficult and there often are diverse valid beliefs\nabout what the correct data labels should be. So far, dataset creators have\nacknowledged annotator subjectivity, but rarely actively managed it in the\nannotation process. This has led to partly-subjective datasets that fail to\nserve a clear downstream use. To address this issue, we propose two contrasting\nparadigms for data annotation. The descriptive paradigm encourages annotator\nsubjectivity, whereas the prescriptive paradigm discourages it. Descriptive\nannotation allows for the surveying and modelling of different beliefs, whereas\nprescriptive annotation enables the training of models that consistently apply\none belief. We discuss benefits and challenges in implementing both paradigms,\nand argue that dataset creators should explicitly aim for one or the other to\nfacilitate the intended use of their dataset. Lastly, we conduct an annotation\nexperiment using hate speech data that illustrates the contrast between the two\nparadigms.",
      "authors": [
        "Paul Röttger",
        "Bertie Vidgen",
        "Dirk Hovy",
        "Janet B. Pierrehumbert"
      ],
      "year": 2021,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2112.07475v2",
      "pdf_url": "http://arxiv.org/pdf/2112.07475v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2501.18474v1",
      "source": "arxiv",
      "title": "Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for\n  VFSS Segmentations",
      "abstract": "Vision foundation models have demonstrated exceptional generalization\ncapabilities in segmentation tasks for both generic and specialized images.\nHowever, a performance gap persists between foundation models and\ntask-specific, specialized models. Fine-tuning foundation models on downstream\ndatasets is often necessary to bridge this gap. Unfortunately, obtaining fully\nannotated ground truth for downstream datasets is both challenging and costly.\nTo address this limitation, we propose a novel test-time training paradigm that\nenhances the performance of foundation models on downstream datasets without\nrequiring full annotations. Specifically, our method employs simple point\nprompts to guide a test-time semi-self-supervised training task. The model\nlearns by resolving the ambiguity of the point prompt through various\naugmentations. This approach directly tackles challenges in the medical imaging\nfield, where acquiring annotations is both time-intensive and expensive. We\nconducted extensive experiments on our new Videofluoroscopy dataset (VFSS-5k)\nfor the instance segmentation task, achieving an average Dice coefficient of\n0.868 across 12 anatomies with a single model.",
      "authors": [
        "Chengxi Zeng",
        "David Smithard",
        "Alberto M Gambaruto",
        "Tilo Burghardt"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2501.18474v1",
      "pdf_url": "http://arxiv.org/pdf/2501.18474v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2508.21096v1",
      "source": "arxiv",
      "title": "ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset\n  for Laparoscopic Surgical Instruments",
      "abstract": "Localisation of surgical tools constitutes a foundational building block for\ncomputer-assisted interventional technologies. Works in this field typically\nfocus on training deep learning models to perform segmentation tasks.\nPerformance of learning-based approaches is limited by the availability of\ndiverse annotated data. We argue that skeletal pose annotations are a more\nefficient annotation approach for surgical tools, striking a balance between\nrichness of semantic information and ease of annotation, thus allowing for\naccelerated growth of available annotated data. To encourage adoption of this\nannotation style, we present, ROBUST-MIPS, a combined tool pose and tool\ninstance segmentation dataset derived from the existing ROBUST-MIS dataset. Our\nenriched dataset facilitates the joint study of these two annotation styles and\nallow head-to-head comparison on various downstream tasks. To demonstrate the\nadequacy of pose annotations for surgical tool localisation, we set up a simple\nbenchmark using popular pose estimation methods and observe high-quality\nresults. To ease adoption, together with the dataset, we release our benchmark\nmodels and custom tool pose annotation software.",
      "authors": [
        "Zhe Han",
        "Charlie Budd",
        "Gongyu Zhang",
        "Huanyu Tian",
        "Christos Bergeles",
        "Tom Vercauteren"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2508.21096v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21096v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2505.23726v1",
      "source": "arxiv",
      "title": "FMG-Det: Foundation Model Guided Robust Object Detection",
      "abstract": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches.",
      "authors": [
        "Darryl Hannan",
        "Timothy Doster",
        "Henry Kvinge",
        "Adam Attarian",
        "Yijing Watkins"
      ],
      "year": 2025,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2505.23726v1",
      "pdf_url": "http://arxiv.org/pdf/2505.23726v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40894586",
      "source": "pubmed",
      "title": "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis.",
      "abstract": null,
      "authors": [
        "Liu T",
        "Chen T",
        "Zheng W",
        "Luo X",
        "Chen Y",
        "Zhao H"
      ],
      "year": 2025,
      "venue": "bioRxiv : the preprint server for biology",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40894586/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "40427668",
      "source": "pubmed",
      "title": "FPCAM: A Weighted Dictionary-Driven Model for Single-Cell Annotation in Pulmonary Fibrosis.",
      "abstract": null,
      "authors": [
        "Liu G",
        "Shi Y",
        "Huang H",
        "Xiao N",
        "Liu C",
        "Zhao H",
        "Xing Y",
        "Cai L"
      ],
      "year": 2025,
      "venue": "Biology",
      "doi": null,
      "url": "https://pubmed.ncbi.nlm.nih.gov/40427668/",
      "pdf_url": null,
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2405.20109v2",
      "source": "arxiv",
      "title": "FMARS: Annotating Remote Sensing Images for Disaster Management using\n  Foundation Models",
      "abstract": "Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and Segment Anything (SAM) provide\nopportunities to automatically generate annotations. This study introduces\nFMARS (Foundation Model Annotations in Remote Sensing), a methodology\nleveraging VHR imagery and foundation models for fast and robust annotation. We\nfocus on disaster management and provide a large-scale dataset with labels\nobtained from pre-event imagery over 19 disaster events, derived from the Maxar\nOpen Data initiative. We train segmentation models on the generated labels,\nusing Unsupervised Domain Adaptation (UDA) techniques to increase\ntransferability to real-world scenarios. Our results demonstrate the\neffectiveness of leveraging foundation models to automatically annotate remote\nsensing data at scale, enabling robust downstream models for critical\napplications. Code and dataset are available at\n\\url{https://github.com/links-ads/igarss-fmars}.",
      "authors": [
        "Edoardo Arnaudo",
        "Jacopo Lungo Vaschetti",
        "Lorenzo Innocenti",
        "Luca Barco",
        "Davide Lisi",
        "Vanina Fissore",
        "Claudio Rossi"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2405.20109v2",
      "pdf_url": "http://arxiv.org/pdf/2405.20109v2",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2404.13239v1",
      "source": "arxiv",
      "title": "Beyond Pixel-Wise Supervision for Medical Image Segmentation: From\n  Traditional Models to Foundation Models",
      "abstract": "Medical image segmentation plays an important role in many image-guided\nclinical approaches. However, existing segmentation algorithms mostly rely on\nthe availability of fully annotated images with pixel-wise annotations for\ntraining, which can be both labor-intensive and expertise-demanding, especially\nin the medical imaging domain where only experts can provide reliable and\naccurate annotations. To alleviate this challenge, there has been a growing\nfocus on developing segmentation methods that can train deep models with weak\nannotations, such as image-level, bounding boxes, scribbles, and points. The\nemergence of vision foundation models, notably the Segment Anything Model\n(SAM), has introduced innovative capabilities for segmentation tasks using weak\nannotations for promptable segmentation enabled by large-scale pre-training.\nAdopting foundation models together with traditional learning methods has\nincreasingly gained recent interest research community and shown potential for\nreal-world applications. In this paper, we present a comprehensive survey of\nrecent progress on annotation-efficient learning for medical image segmentation\nutilizing weak annotations before and in the era of foundation models.\nFurthermore, we analyze and discuss several challenges of existing approaches,\nwhich we believe will provide valuable guidance for shaping the trajectory of\nfoundational models to further advance the field of medical image segmentation.",
      "authors": [
        "Yuyan Shi",
        "Jialu Ma",
        "Jin Yang",
        "Shasha Wang",
        "Yichi Zhang"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2404.13239v1",
      "pdf_url": "http://arxiv.org/pdf/2404.13239v1",
      "citations_count": null,
      "keywords": []
    },
    {
      "id": "http://arxiv.org/abs/2404.11957v1",
      "source": "arxiv",
      "title": "The devil is in the object boundary: towards annotation-free instance\n  segmentation using Foundation Models",
      "abstract": "Foundation models, pre-trained on a large amount of data have demonstrated\nimpressive zero-shot capabilities in various downstream tasks. However, in\nobject detection and instance segmentation, two fundamental computer vision\ntasks heavily reliant on extensive human annotations, foundation models such as\nSAM and DINO struggle to achieve satisfactory performance. In this study, we\nreveal that the devil is in the object boundary, \\textit{i.e.}, these\nfoundation models fail to discern boundaries between individual objects. For\nthe first time, we probe that CLIP, which has never accessed any instance-level\nannotations, can provide a highly beneficial and strong instance-level boundary\nprior in the clustering results of its particular intermediate layer. Following\nthis surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up\nCL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery\npipeline, enabling annotation-free, complex-scene-capable, open-vocabulary\nobject detection and instance segmentation. Our Zip significantly boosts SAM's\nmask AP on COCO dataset by 12.5% and establishes state-of-the-art performance\nin various settings, including training-free, self-training, and\nlabel-efficient finetuning. Furthermore, annotation-free Zip even achieves\ncomparable performance to the best-performing open-vocabulary object detecters\nusing base annotations. Code is released at\nhttps://github.com/ChengShiest/Zip-Your-CLIP",
      "authors": [
        "Cheng Shi",
        "Sibei Yang"
      ],
      "year": 2024,
      "venue": "arXiv",
      "doi": null,
      "url": "http://arxiv.org/abs/2404.11957v1",
      "pdf_url": "http://arxiv.org/pdf/2404.11957v1",
      "citations_count": null,
      "keywords": []
    }
  ],
  "reviews": [
    {
      "paper_id": "http://arxiv.org/abs/2507.02743v1",
      "summary": {
        "tldr": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acq",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2502.03907v1",
      "summary": {
        "tldr": "We analyze the capabilities of foundation models addressing the tedious task\nof generating annotations for animal tracking. Annotating a large amount of\ndata is vital and can be a make-or-break factor for the robustness of a\ntracking model. Robustness is particularly crucial in a...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "We analyze the capabilities of foundation models addressing the tedious task\nof generating annotations for animal tracking. Annotating a large amount of\ndata is vital and can be a make-or-break factor",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2503.09860v1",
      "summary": {
        "tldr": "Developing robust and versatile deep-learning models is essential for\nenhancing diagnostic accuracy and guiding clinical interventions in medical\nimaging, but it requires a large amount of annotated data. The advancement of\ndeep learning has facilitated the creation of numerous m...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Developing robust and versatile deep-learning models is essential for\nenhancing diagnostic accuracy and guiding clinical interventions in medical\nimaging, but it requires a large amount of annotated d",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2506.02914v1",
      "summary": {
        "tldr": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To stud...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-cra",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2508.15751v1",
      "summary": {
        "tldr": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentat",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40537825",
      "summary": {
        "tldr": "scExtract: leveraging large language models for fully automated single-cell RNA-seq data annotation and prior-informed multi-dataset integration.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2403.09307v3",
      "summary": {
        "tldr": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zeroshot semant...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel level\nannotations. With the success of foundation models and es",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2403.15218v1",
      "summary": {
        "tldr": "Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Mod...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited ",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2403.07687v1",
      "summary": {
        "tldr": "Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geograph",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2401.04464v2",
      "summary": {
        "tldr": "Massive amounts of unlabelled data are captured by Earth Observation (EO)\nsatellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.\nThis makes Remote Sensing a data-rich domain well suited to Machine Learning\n(ML) solutions. However, a bottleneck in applying ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Massive amounts of unlabelled data are captured by Earth Observation (EO)\nsatellites, with the Sentinel-2 constellation generating 1. 6 TB of data daily",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2406.18239v1",
      "summary": {
        "tldr": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Nat...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2402.06560v1",
      "summary": {
        "tldr": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain expert...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, oft",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2402.08075v1",
      "summary": {
        "tldr": "Although DNA foundation models have advanced the understanding of genomes,\nthey still face significant challenges in the limited scale and diversity of\ngenomic data. This limitation starkly contrasts with the success of natural\nlanguage foundation models, which thrive on substant...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Although DNA foundation models have advanced the understanding of genomes,\nthey still face significant challenges in the limited scale and diversity of\ngenomic data. This limitation starkly contrasts ",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2408.05618v1",
      "summary": {
        "tldr": "Retinal foundation models aim to learn generalizable representations from\ndiverse retinal images, facilitating label-efficient model adaptation across\nvarious ophthalmic tasks. Despite their success, current retinal foundation\nmodels are generally restricted to a single imaging m...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Retinal foundation models aim to learn generalizable representations from\ndiverse retinal images, facilitating label-efficient model adaptation across\nvarious ophthalmic tasks. Despite their success, ",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2309.08108v1",
      "summary": {
        "tldr": "Significant advances are being made in speech emotion recognition (SER) using\ndeep learning models. Nonetheless, training SER systems remains challenging,\nrequiring both time and costly resources. Like many other machine learning\ntasks, acquiring datasets for SER requires substan...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Significant advances are being made in speech emotion recognition (SER) using\ndeep learning models. Nonetheless, training SER systems remains challenging,\nrequiring both time and costly resources",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2311.08077v2",
      "summary": {
        "tldr": "The advent of foundation models signals a new era in artificial intelligence.\nThe Segment Anything Model (SAM) is the first foundation model for image\nsegmentation. In this study, we evaluate SAM's ability to segment features from\neye images recorded in virtual reality setups. Th...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "The advent of foundation models signals a new era in artificial intelligence. The Segment Anything Model (SAM) is the first foundation model for image\nsegmentation",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "overclaiming",
            "severity": "medium",
            "rationale": "Claims strong superiority; check against baselines in paper."
          },
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2112.07475v2",
      "summary": {
        "tldr": "Labelled data is the foundation of most natural language processing tasks.\nHowever, labelling data is difficult and there often are diverse valid beliefs\nabout what the correct data labels should be. So far, dataset creators have\nacknowledged annotator subjectivity, but rarely ac...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs\nabout what the correct data labels should be",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2501.18474v1",
      "summary": {
        "tldr": "Vision foundation models have demonstrated exceptional generalization\ncapabilities in segmentation tasks for both generic and specialized images.\nHowever, a performance gap persists between foundation models and\ntask-specific, specialized models. Fine-tuning foundation models on ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Vision foundation models have demonstrated exceptional generalization\ncapabilities in segmentation tasks for both generic and specialized images. However, a performance gap persists between foundation",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2508.21096v1",
      "summary": {
        "tldr": "Localisation of surgical tools constitutes a foundational building block for\ncomputer-assisted interventional technologies. Works in this field typically\nfocus on training deep learning models to perform segmentation tasks.\nPerformance of learning-based approaches is limited by t...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Localisation of surgical tools constitutes a foundational building block for\ncomputer-assisted interventional technologies. Works in this field typically\nfocus on training deep learning models to perf",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2505.23726v1",
      "summary": {
        "tldr": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent ",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40894586",
      "summary": {
        "tldr": "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "40427668",
      "summary": {
        "tldr": "FPCAM: A Weighted Dictionary-Driven Model for Single-Cell Annotation in Pulmonary Fibrosis.",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.0,
        "quotes": []
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2405.20109v2",
      "summary": {
        "tldr": "Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and Segment Anything (SAM) provide\nopportunities to automatically generate annotation...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Very-High Resolution (VHR) remote sensing imagery is increasingly accessible,\nbut often lacks annotations for effective machine learning applications. Recent\nfoundation models like GroundingDINO and S",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2404.13239v1",
      "summary": {
        "tldr": "Medical image segmentation plays an important role in many image-guided\nclinical approaches. However, existing segmentation algorithms mostly rely on\nthe availability of fully annotated images with pixel-wise annotations for\ntraining, which can be both labor-intensive and experti...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Medical image segmentation plays an important role in many image-guided\nclinical approaches. However, existing segmentation algorithms mostly rely on\nthe availability of fully annotated images with pi",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    },
    {
      "paper_id": "http://arxiv.org/abs/2404.11957v1",
      "summary": {
        "tldr": "Foundation models, pre-trained on a large amount of data have demonstrated\nimpressive zero-shot capabilities in various downstream tasks. However, in\nobject detection and instance segmentation, two fundamental computer vision\ntasks heavily reliant on extensive human annotations, ...",
        "methods": null,
        "results": null,
        "limitations": null,
        "citations": null,
        "grounding_score": 0.2,
        "quotes": [
          {
            "text": "Foundation models, pre-trained on a large amount of data have demonstrated\nimpressive zero-shot capabilities in various downstream tasks. However, in\nobject detection and instance segmentation, two fu",
            "section": "abstract"
          }
        ]
      },
      "critique": {
        "issues": [
          {
            "tag": "missing_baselines",
            "severity": "low",
            "rationale": "Results do not clearly compare against established baselines."
          },
          {
            "tag": "reproducibility",
            "severity": "medium",
            "rationale": "No mention of code/data availability or reproducibility."
          }
        ],
        "overall_note": null
      }
    }
  ],
  "matrix": {
    "columns": [
      "Venue",
      "Year",
      "Citations",
      "Methods",
      "Results",
      "Limitations"
    ],
    "rows": [
      "Prompt learning with bounding box constraints for medical image\n  segmentation",
      "No Free Lunch in Annotation either: An objective evaluation of\n  foundation models for streamlining annotation in animal tracking",
      "Foundation X: Integrating Classification, Localization, and Segmentation\n  through Lock-Release Pretraining Strategy for Chest X-ray Analysis",
      "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through\n  3D LiDAR Detection",
      "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model",
      "scExtract: leveraging large language models for fully automated single-cell RNA-seq data annotation and prior-informed multi-dataset integration.",
      "Annotation Free Semantic Segmentation with Vision Foundation Models",
      "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations",
      "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost",
      "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
      "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets",
      "Video Annotator: A framework for efficiently building video classifiers\n  using vision-language models and active learning",
      "Efficient and Scalable Fine-Tune of Language Models for Genome\n  Understanding",
      "UrFound: Towards Universal Retinal Foundation Models via\n  Knowledge-Guided Masked Modeling",
      "Foundation Model Assisted Automatic Speech Emotion Recognition:\n  Transcribing, Annotating, and Augmenting",
      "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model\n  (SAM)",
      "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks",
      "Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for\n  VFSS Segmentations",
      "ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset\n  for Laparoscopic Surgical Instruments",
      "FMG-Det: Foundation Model Guided Robust Object Detection",
      "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis.",
      "FPCAM: A Weighted Dictionary-Driven Model for Single-Cell Annotation in Pulmonary Fibrosis.",
      "FMARS: Annotating Remote Sensing Images for Disaster Management using\n  Foundation Models",
      "Beyond Pixel-Wise Supervision for Medical Image Segmentation: From\n  Traditional Models to Foundation Models",
      "The devil is in the object boundary: towards annotation-free instance\n  segmentation using Foundation Models"
    ],
    "data": {
      "Prompt learning with bounding box constraints for medical image\n  segmentation": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "No Free Lunch in Annotation either: An objective evaluation of\n  foundation models for streamlining annotation in animal tracking": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Foundation X: Integrating Classification, Localization, and Segmentation\n  through Lock-Release Pretraining Strategy for Chest X-ray Analysis": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through\n  3D LiDAR Detection": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "scExtract: leveraging large language models for fully automated single-cell RNA-seq data annotation and prior-informed multi-dataset integration.": {
        "Venue": {
          "value": "Genome biology"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Annotation Free Semantic Segmentation with Vision Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "PhilEO Bench: Evaluating Geo-Spatial Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Video Annotator: A framework for efficiently building video classifiers\n  using vision-language models and active learning": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Efficient and Scalable Fine-Tune of Language Models for Genome\n  Understanding": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "UrFound: Towards Universal Retinal Foundation Models via\n  Knowledge-Guided Masked Modeling": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Foundation Model Assisted Automatic Speech Emotion Recognition:\n  Transcribing, Annotating, and Augmenting": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model\n  (SAM)": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2023"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2021"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for\n  VFSS Segmentations": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset\n  for Laparoscopic Surgical Instruments": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "FMG-Det: Foundation Model Guided Robust Object Detection": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis.": {
        "Venue": {
          "value": "bioRxiv : the preprint server for biology"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "FPCAM: A Weighted Dictionary-Driven Model for Single-Cell Annotation in Pulmonary Fibrosis.": {
        "Venue": {
          "value": "Biology"
        },
        "Year": {
          "value": "2025"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "FMARS: Annotating Remote Sensing Images for Disaster Management using\n  Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "Beyond Pixel-Wise Supervision for Medical Image Segmentation: From\n  Traditional Models to Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      },
      "The devil is in the object boundary: towards annotation-free instance\n  segmentation using Foundation Models": {
        "Venue": {
          "value": "arXiv"
        },
        "Year": {
          "value": "2024"
        },
        "Citations": {
          "value": ""
        },
        "Methods": {
          "value": ""
        },
        "Results": {
          "value": ""
        },
        "Limitations": {
          "value": ""
        }
      }
    }
  },
  "synthesis": {
    "executive_summary": "Reviewed 25 papers across 4 venues. We summarize methods, results, and limitations, and identify common gaps.",
    "gaps": [
      "Few works report comparisons against strong baselines."
    ],
    "future_work": [
      "Open-sourcing code and datasets",
      "Larger, diverse cohorts",
      "Robust baseline comparisons"
    ]
  },
  "artifacts": {
    "markdown_path": "outputs\\review_foundation-models-for-single-cell-annotation.md",
    "json_path": "outputs\\review_foundation-models-for-single-cell-annotation.json",
    "csv_path": "outputs\\papers_foundation-models-for-single-cell-annotation.csv"
  },
  "created_at": "2025-09-13T05:55:18.385575"
}